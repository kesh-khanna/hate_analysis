# -*- coding: utf-8 -*-
"""BerkleyLSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uq0stAf3RwtkcBDvfvp81ao6REtHnNgX
"""

# !pip install numpy
# !pip install tensorflow
# !pip install pandas
# !pip install scikit-learn
# !pip install seaborn
# !pip install matplotlib
# !pip install datasets

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import sklearn
import seaborn
import matplotlib as plt
import datasets
import keras
import re

def download_data():
    dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech')
    print(dataset)
    df = dataset['train'].to_pandas()
    df.describe()
    return df

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def preprocess_text(text):
    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    # Remove mentions and hashtags
    text = re.sub(r'\@\w+|\#', '', text)
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)

def preprocess(binary=True):
    df = download_data()
    df = df[['text', 'hate_speech_score', 'annotator_severity']]
    df = df.dropna()

    # Apply preprocessing to text
    df['text'] = df['text'].apply(preprocess_text)

    if binary:
      df_offensive = df[df['hate_speech_score'] < 0.5]
      df_offensive.loc[:, 'label'] = 0
      df_hate = df[df['hate_speech_score'] > 0.5]
      df_hate.loc[:, 'label'] = 1

    # combine the two dataframes
      df = pd.concat([df_offensive, df_hate])
      df = df.sample(frac=1).reset_index(drop=True)
    else:
      #Split for multi class labels : supportive, offensive, hate speech
      df_support = df[df['hate_speech_score'] < -1]
      df_offensive = df[(df['hate_speech_score'] < 0.5) & (df['hate_speech_score'] >= -1)]

      df_hate = df[df['hate_speech_score'] >= 0.5]

      df_support.loc[:, 'label'] = 0
      df_offensive.loc[:, 'label'] = 1
      df_hate.loc[:, 'label'] = 2

      df = pd.concat([df_support, df_offensive, df_hate])
      df = df.sample(frac=1).reset_index(drop=True)

    return df

df = preprocess(binary=True)

import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split


X = df['text']
y = df['label']

tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
padded_sequences = pad_sequences(sequences, maxlen=200)

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)

from sklearn.utils.class_weight import compute_class_weight
y_train_integers = y_train.astype(int)

# Calculate class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_integers),
    y=y_train_integers
)

# Convert class weights to a dictionary to pass to Keras
class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}

from keras.layers import Bidirectional
from keras.layers import Dropout

def calculate_vocab_size(dataset):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(dataset['text'])
    return len(tokenizer.word_index)


vocab_size = calculate_vocab_size(df)

from keras.callbacks import EarlyStopping, ModelCheckpoint

from keras.models import load_model

# Load GloVe embeddings
def load_glove_embeddings(glove_file):
    embeddings_index = {}
    with open(glove_file, encoding='utf8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coeffs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coeffs
    return embeddings_index

embeddings_index = load_glove_embeddings('glove.twitter.27B.200d.txt')  # Change file name/path as necessary

# Create embedding matrix
embedding_dim = 200  
vocab_size = len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

from keras.models import load_model
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from keras.utils import plot_model

from keras.callbacks import Callback
from keras.layers import BatchNormalization
from keras.layers import Dropout
from keras.regularizers import l2
from keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=200,
                    weights=[embedding_matrix], trainable=False))
model.add(Bidirectional(LSTM(128, return_sequences=True)))
model.add(Bidirectional(LSTM(128)))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid')) 


model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)



# Compile the Model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


checkpoint_filepath = '/content/bestmodel2.h5'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True,
    verbose=1  # Add verbose for logging the saving of checkpoints
    # Ensure no unsupported parameters are included
)

# Model is saved at the end of every epoch, if it's the best seen so far.
model.fit(X_train, y_train, epochs=10,validation_data=(X_val, y_val), callbacks=[model_checkpoint_callback])

best_model = load_model('bestmodel2.h5')

# Predictions
y_pred = best_model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype("int32")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_classes)
print("Confusion Matrix:\n", cm)

# Classification Report
cr = classification_report(y_test, y_pred_classes)
print("\nClassification Report:\n", cr)

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
auc = roc_auc_score(y_test, y_pred)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'AUC = {auc:.2f}')
plt.plot([0, 1], [0, 1], color='darkgrey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()


plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)

from sklearn.preprocessing import OneHotEncoder

df = preprocess(binary=False)

# OneHotEncoder for the labels
encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
encoded_labels = encoder.fit_transform(df[['label']])
encoded_df = pd.DataFrame(encoded_labels, columns=encoder.get_feature_names_out(['label']))

# Concatenate the encoded labels with the original DataFrame
df = pd.concat([df, encoded_df], axis=1).drop('label', axis=1)

df = df.rename(columns={
    'label_0': 'support',
    'label_1': 'offensive',
    'label_2': 'hate'
})

class_distribution = df[['support', 'offensive', 'hate']].sum().to_frame(name='count')
print(class_distribution)

from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

X = df['text']
y = df[['support', 'offensive', 'hate']].values
y_integers = np.argmax(y, axis=1)
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers)
class_weight_dict = dict(enumerate(class_weights))


print(class_weight_dict)

tokenizer = Tokenizer(num_words=None, oov_token="<OOV>")
tokenizer.fit_on_texts(df['text'])
vocab_size = len(tokenizer.word_index) + 1  

print(f"Vocabulary size: {vocab_size}")

sequences = tokenizer.texts_to_sequences(df['text'])
# Calculate sequence lengths
sequence_lengths = [len(seq) for seq in sequences]

# Plotting the distribution of sequence lengths
plt.hist(sequence_lengths, bins=30)
plt.title('Distribution of Sequence Lengths')
plt.xlabel('Sequence Length')
plt.ylabel('Frequency')
plt.show()

# Calculate the 95th percentile of sequence lengths
percentile_95 = np.percentile(sequence_lengths, 95)
print(f"The 95th percentile of sequence lengths is: {percentile_95}")
padded_sequences = pad_sequences(sequences, maxlen=int(percentile_95))

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df[['support', 'offensive', 'hate']], test_size=0.2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)

# Step 3: Loading GloVe Embeddings
def load_glove_embeddings(filepath):
    embeddings_index = {}
    with open(filepath) as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

embeddings_index = load_glove_embeddings('glove.twitter.27B.200d.txt')  # Path to GloVe file

embedding_dim = 200  # Dimension of GloVe vectors you are using
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in tokenizer.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

print(y_train['offensive'].sum())

from keras.callbacks import Callback
from keras.layers import BatchNormalization
from keras.layers import Dropout
from keras.regularizers import l2
from keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True))
model.add(Bidirectional(LSTM(256, return_sequences=True)))
model.add(BatchNormalization())
model.add(Bidirectional(LSTM(256, return_sequences=True)))
model.add(BatchNormalization())
model.add(Bidirectional(LSTM(256)))
model.add(BatchNormalization())
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))  # Assuming 3 classes

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, mode='min')
lr_scheduler = LearningRateScheduler(lambda epoch, lr: lr * 0.9 if epoch > 1 else lr)

checkpoint_filepath = '/content/checkpoint_multi.h5'
model_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min', save_best_only=True, verbose=1)

# Model Compilation
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy' , metrics=['accuracy'])

class Metrics(Callback):
    def __init__(self, train_data, validation_data):
        super(Metrics, self).__init__()
        self.train_data = train_data
        self.validation_data = validation_data

    def on_epoch_end(self, epoch, logs=None):
        train_pred = np.argmax(self.model.predict(self.train_data[0]), axis=1)
        train_true = np.argmax(self.train_data[1], axis=1)
        val_pred = np.argmax(self.model.predict(self.validation_data[0]), axis=1)
        val_true = np.argmax(self.validation_data[1], axis=1)

        print(f'\nMetrics for Epoch {epoch+1}')
        print('\nTraining Data:')
        print(classification_report(train_true, train_pred, target_names=['Support', 'Offensive', 'Hate']))
        print('\nValidation Data:')
        print(classification_report(val_true, val_pred, target_names=['Support', 'Offensive', 'Hate']))

# Usage
metrics = Metrics(train_data=(X_train, y_train), validation_data=(X_val, y_val))

# Model Training
model.fit(X_train, y_train, epochs=15, validation_data=(X_val, y_val), class_weight=class_weight_dict, callbacks=[early_stopping, lr_scheduler, model_checkpoint])

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize


# Make predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels

# Convert y_test from one-hot encoded to single integer labels
y_test_classes = np.argmax(y_test.values, axis=1)

# Calculate Accuracy
accuracy = accuracy_score(y_test_classes, y_pred_classes)

# Generate a classification report
report = classification_report(y_test_classes, y_pred_classes, target_names=['support', 'offensive', 'hate'])

# Generate a confusion matrix
conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)

# Print the metrics
print("Accuracy:", accuracy)
print("\nClassification Report:\n", report)
print("\nConfusion Matrix:\n", conf_matrix)


# Binarize y_test for multi-class AUC calculation
y_test_binarized = label_binarize(y_test_classes, classes=[0, 1, 2])  # Adjust the classes as necessary

# Calculate the AUC for each class and average
auc = []
for i in range(3):  # Assuming three classes
    auc_class_i = roc_auc_score(y_test_binarized[:, i], y_pred[:, i])
    auc.append(auc_class_i)

# Calculate the average AUC
average_auc = np.mean(auc)

# Print the AUC
print("Average AUC:", average_auc)

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt


# Binarize y_test for multi-class ROC curve calculation
y_test_binarized = label_binarize(y_test_classes, classes=[0, 1, 2])  # Adjust the classes as necessary

# Calculate ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
n_classes = y_test_binarized.shape[1]

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot the ROC curve for each class
plt.figure(figsize=(10, 8))
colors = ['blue', 'green', 'red']
class_names = ['support', 'offensive', 'hate']  # adjust as necessary

for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic for Multi-Class')
plt.legend(loc="lower right")
plt.show()